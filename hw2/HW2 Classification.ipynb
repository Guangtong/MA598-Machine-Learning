{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA598 HW 2\n",
    "\n",
    "### Guangtong Shen (0028213116)\n",
    "### Fall 2016\n",
    "\n",
    "## Task\n",
    "\n",
    "use Sklearn.svm.svc to study handwritten digits from the  processed  US  Postal  Service  Zip  Code  data  set.  Download  the  data (the format of each row is: digit intensity symmetry) for training and testing: \n",
    "\n",
    "http://www.amlbook.com/data/zip/features.train \n",
    "\n",
    "http://www.amlbook.com/data/zip/features.test \n",
    "\n",
    " We will train a one-versus-one (one digit is class +1 and another digit is class -1) classifier for the digits '1' (+1) and '5' (-1). \n",
    " \n",
    " \n",
    "## Definition\n",
    "* Ein returns the in training sample error of the current svm model. It is the fraction of in training sample points which got misclassified. \n",
    " \n",
    "* Eout returns the testing sample error of the current svm model. It is the fraction of testing sample points which got misclassified. \n",
    " \n",
    "* Ecv returns the leave one out cross validation in training sample error of the current svm model. \n",
    " \n",
    "* accuracy over the testing set = 1 - Eout  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Read train and test dataset, only keep the rows of digit 1 (labeled 1) and digit 5 (labeled -1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 appears 1005 times in features.train\n",
      "5 appears 0 times in features.train\n",
      "1 appears 264 times in features.test\n",
      "5 appears 0 times in features.test\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def parseDataWithShuffle(filename):\n",
    "    x = []\n",
    "    y = []\n",
    "    xy = []\n",
    "    with open(filename) as file:\n",
    "        for eachline in file:\n",
    "            line = eachline.strip().split('  ')\n",
    "            a,b,c = (int(float(line[0])), float(line[1]), float(line[2]))\n",
    "            if(a == 1 or a == 5):\n",
    "                xy.append([b, c, 1 if a == 1 else -1])\n",
    "\n",
    "    random.shuffle(xy)\n",
    "    x = [[row[0], row[1]] for row in xy]\n",
    "    y = [row[2] for row in xy]\n",
    "    count1 = len([yi for yi in y if yi == 1])\n",
    "    print \"1 appears \" + str(count1) + \" times in \" + filename;\n",
    "    count5 = len([yi for yi in y if yi == 0])\n",
    "    print \"5 appears \" + str(count5) + \" times in \" + filename;\n",
    "    return  np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = parseDataWithShuffle(\"features.train\")\n",
    "x_test, y_test = parseDataWithShuffle(\"features.test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A\n",
    "* Consider the linear kernel K(x_n, x_m) = x_n^T x_m. Train and test using all of the points, writing the output to an output file hw2.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over testing set: 0.978773584906\n",
      "number of support vectors for each class: [14 14]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=0)\n",
    "svm.fit(x_train, y_train)\n",
    "y_predict = svm.predict(x_test)\n",
    "\n",
    "with open('hw2.txt', 'w') as file:\n",
    "    file.write(\"Test set labels \\t Predicted labels\\n\")\n",
    "    for i,j in zip(y_test, y_predict):\n",
    "        file.write(str(i) + '\\t' + str(j) + '\\n')\n",
    "\n",
    "def countError(y_real,y_pred):\n",
    "    if len(y_real) != len(y_pred):\n",
    "        return 0\n",
    "    errors = [1 for i,j in zip(y_real, y_pred) if i != j]\n",
    "    return float(len(errors))/len(y_real)\n",
    "\n",
    "Ein  = countError(y_train, svm.predict(x_train))\n",
    "Eout = countError(y_test, svm.predict(x_test))\n",
    "\n",
    "print \"Accuracy over testing set: \" + str(1-Eout)\n",
    "print \"number of support vectors for each class: \" + str(svm.n_support_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to using all of the training examples, try subsets of the training data and print out accuracy over the testing set (1 - Eout (over all test examples), and the number of support vectors. Try with the first {50, 100, 200, 800} points with the linear kernel. The output of these experiments should be written in Markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50\n",
      "Accuracy over testing set: 0.97641509434\n",
      "number of support vectors for each class: [3 3]\n",
      "Training set size: 100\n",
      "Accuracy over testing set: 0.974056603774\n",
      "number of support vectors for each class: [3 3]\n",
      "Training set size: 200\n",
      "Accuracy over testing set: 0.978773584906\n",
      "number of support vectors for each class: [4 4]\n",
      "Training set size: 800\n",
      "Accuracy over testing set: 0.978773584906\n",
      "number of support vectors for each class: [7 7]\n"
     ]
    }
   ],
   "source": [
    "svm.fit(x_train[0:50], y_train[0:50])\n",
    "Eout = countError(y_test, svm.predict(x_test))\n",
    "print \"Training set size: 50\"\n",
    "print \"Accuracy over testing set: \" + str(1-Eout)\n",
    "print \"number of support vectors for each class: \" + str(svm.n_support_) \n",
    "\n",
    "svm.fit(x_train[0:100], y_train[0:100])\n",
    "Eout = countError(y_test, svm.predict(x_test))\n",
    "print \"Training set size: 100\"\n",
    "print \"Accuracy over testing set: \" + str(1-Eout)\n",
    "print \"number of support vectors for each class: \" + str(svm.n_support_) \n",
    "\n",
    "svm.fit(x_train[0:200], y_train[0:200])\n",
    "Eout = countError(y_test, svm.predict(x_test))\n",
    "print \"Training set size: 200\"\n",
    "print \"Accuracy over testing set: \" + str(1-Eout)\n",
    "print \"number of support vectors for each class: \" + str(svm.n_support_) \n",
    "\n",
    "svm.fit(x_train[0:800], y_train[0:800])\n",
    "Eout = countError(y_test, svm.predict(x_test))\n",
    "print \"Training set size: 800\"\n",
    "print \"Accuracy over testing set: \" + str(1-Eout)\n",
    "print \"number of support vectors for each class: \" + str(svm.n_support_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question B\n",
    "\n",
    "* Consider the polynomial kernel K(x_n, x_m) = (1 + x_n^T x_m)^Q, where Q is the degree of the polynomial. \n",
    "* Comparing Q = 2 with Q = 5, which of the following statements is correct? \n",
    "    1. When C = 0.0001, Ein is higher at Q = 5. \n",
    "    2. When C = 0.001, the number of support vectors is lower at Q = 5. \n",
    "    3. When C = 0.01, Ein is higher at Q = 5. \n",
    "    4. When C = 1, Eout is lower at Q = 5. \n",
    "    5. None of the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C = 0.0001\n",
      "degree = 2 \t Ein = 0.022421525 \t Eout = 0.030660377 \t NumSV = 255\n",
      "degree = 5 \t Ein = 0.006406150 \t Eout = 0.018867925 \t NumSV = 21\n",
      "\n",
      "C = 0.001\n",
      "degree = 2 \t Ein = 0.007046765 \t Eout = 0.018867925 \t NumSV = 76\n",
      "degree = 5 \t Ein = 0.005124920 \t Eout = 0.016509434 \t NumSV = 14\n",
      "\n",
      "C = 0.01\n",
      "degree = 2 \t Ein = 0.004484305 \t Eout = 0.018867925 \t NumSV = 27\n",
      "degree = 5 \t Ein = 0.004484305 \t Eout = 0.016509434 \t NumSV = 13\n",
      "\n",
      "C = 1.0\n",
      "degree = 2 \t Ein = 0.004484305 \t Eout = 0.018867925 \t NumSV = 13\n",
      "degree = 5 \t Ein = 0.004484305 \t Eout = 0.016509434 \t NumSV = 13\n"
     ]
    }
   ],
   "source": [
    "template = 'degree = {0} \\t Ein = {1:10.9f} \\t Eout = {2:10.9f} \\t NumSV = {3}';\n",
    "\n",
    "for c_candidate in [0.0001, 0.001, 0.01, 1.0]:\n",
    "    print '\\nC = ' + str(c_candidate);\n",
    "    for d_candidate in [2,5]:\n",
    "        svm = SVC(kernel='poly', C=c_candidate, degree = d_candidate, random_state=0)\n",
    "        svm.fit(x_train, y_train)\n",
    "        Ein = countError(y_train, svm.predict(x_train));\n",
    "        Eout = countError(y_test, svm.predict(x_test));\n",
    "        NumSv = svm.n_support_[0]\n",
    "        print template.format(d_candidate, Ein, Eout, NumSv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, these statements are right:\n",
    "\n",
    "    2. When C = 0.001, the number of support vectors is lower at Q = 5. \n",
    "    4. When C = 1, Eout is lower at Q = 5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question C\n",
    "* Consider  the  1 versus  5 classifier with  Q  = 2 and  C  ∈  {0.001, 0.01, 0.1, 1}. \n",
    "* Which of the following statements is correct?  Going up or down means strictly so. \n",
    " \n",
    "    [a] The number of support vectors goes down when C goes up.  \n",
    "    [b] The number of support vectors goes up when C goes up.  \n",
    "    [c] Eout goes down when C goes up.  \n",
    "    [d] Maximum C achieves the lowest Ein.   \n",
    "    [e] None of the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C = 0.001\n",
      "degree = 2 \t Ein = 0.007046765 \t Eout = 0.018867925 \t NumSV = 76\n",
      "\n",
      "C = 0.01\n",
      "degree = 2 \t Ein = 0.004484305 \t Eout = 0.018867925 \t NumSV = 27\n",
      "\n",
      "C = 0.1\n",
      "degree = 2 \t Ein = 0.004484305 \t Eout = 0.018867925 \t NumSV = 14\n",
      "\n",
      "C = 1.0\n",
      "degree = 2 \t Ein = 0.004484305 \t Eout = 0.018867925 \t NumSV = 13\n"
     ]
    }
   ],
   "source": [
    "for c_candidate in [0.001, 0.01, 0.1, 1.0]:\n",
    "    print '\\nC = ' + str(c_candidate);\n",
    "    svm = SVC(kernel='poly', C=c_candidate, degree = 2, random_state=0)\n",
    "    svm.fit(x_train, y_train)\n",
    "    Ein = countError(y_train, svm.predict(x_train));\n",
    "    Eout = countError(y_test, svm.predict(x_test));\n",
    "    NumSv = svm.n_support_[0]\n",
    "    \n",
    "    print template.format(2, Ein, Eout, NumSv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, these statements are right:  \n",
    "    [a] The number of support vectors goes down when C goes up.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation  \n",
    "In the next two problems, we will experiment with 10-fold cross validation for the polynomial kernel. Because Ecv is a random  variable  that depends  on  the  random  partition  of  the  data,  we  will  try100  runs  with  different partitions and  base our answer on how many runs lead to a particular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question D\n",
    "\n",
    "* Consider the 1 versus 5 classifier with Q = 2.   We use Ecv to select C ∈ {0.0001, 0.001, 0.01, 0.1, 1}. If there is a tie in Ecv, select the smaller C. \n",
    "* Within the 100 random runs, which of the following statements is correct? \n",
    " \n",
    "    [a]  C = 0.0001 is selected most often.   \n",
    "    [b]  C = 0.001 is selected most often.   \n",
    "    [c]  C = 0.01 is selected most often.   \n",
    "    [d]  C = 0.1 is selected most often.   \n",
    "    [e]  C = 1 is selected most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 13, 0.001: 0, 0.0001: 0, 0.1: 4, 0.01: 83}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "def calcEcv(X,Y,c_candidate, random): # small C to tolerate error thus to avoid overfitting\n",
    "    EcvList = []\n",
    "    svm = SVC(kernel='poly', C=c_candidate, degree = 2, random_state=random)\n",
    "    kfold = StratifiedKFold(y=Y, n_folds=10, shuffle=True, random_state=random)\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        svm.fit(X[train], Y[train])\n",
    "        Ecv = countError(Y[test], svm.predict(X[test]))\n",
    "        EcvList.append(Ecv)\n",
    "    return sum(EcvList)/len(EcvList)\n",
    "\n",
    "C_appear_times = {}\n",
    "C_candidates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "for C in C_candidates:\n",
    "    C_appear_times[C] = 0\n",
    "    \n",
    "for i in xrange(100):\n",
    "    minEcv = 1.0\n",
    "    C_selection = 0.0001\n",
    "    EcvOfCandidates = [calcEcv(x_train, y_train, c, i) for c in C_candidates]\n",
    "    C_selection = C_candidates[np.argmin(EcvOfCandidates)]\n",
    "    C_appear_times[C_selection] = C_appear_times[C_selection] + 1\n",
    "\n",
    "print C_appear_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimun Ecv is obtained when C=0.01 for 83 times  \n",
    "So the answer is\n",
    "\n",
    "    [c] C = 0.01 is selected most often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question E\n",
    "* Again, consider the 1 versus 5 classifier with Q = 2. \n",
    "* For the winning selection in the previous problem, the average value of Ecv over the 100 runs is closest to   \n",
    "    [a] 0.001   \n",
    "    [b] 0.003  \n",
    "    [c] 0.005  \n",
    "    [d] 0.007  \n",
    "    [e] 0.009  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00449075827262\n"
     ]
    }
   ],
   "source": [
    "EcvList = [calcEcv(x_train, y_train, 0.01, i) for i in range(100)]\n",
    "avgEcv = sum(EcvList)/len(EcvList)\n",
    "print avgEcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So the average Ecv is closet to\n",
    "\n",
    "    [c] 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questiion F\n",
    "\n",
    "* Consider the radial basis function (RBF) kernel K(x_n, x_m) = e^(- ||xn - xm||^2) in the SVC approach.\n",
    "* Which value of C ∈ {0.01, 1, 100, 10^4, 10^6} results in the lowest Ein? The lowest Eout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C = 0.01\n",
      "Ein = 0.003843690 \t Eout = 0.021226415 \t NumSV = 174\n",
      "\n",
      "C = 1\n",
      "Ein = 0.004484305 \t Eout = 0.021226415 \t NumSV = 18\n",
      "\n",
      "C = 100\n",
      "Ein = 0.003203075 \t Eout = 0.018867925 \t NumSV = 11\n",
      "\n",
      "C = 10000.0\n",
      "Ein = 0.002562460 \t Eout = 0.018867925 \t NumSV = 10\n",
      "\n",
      "C = 1000000.0\n",
      "Ein = 0.001281230 \t Eout = 0.021226415 \t NumSV = 11\n"
     ]
    }
   ],
   "source": [
    "template = 'Ein = {0:10.9f} \\t Eout = {1:10.9f} \\t NumSV = {2}';\n",
    "\n",
    "for c_candidate in [0.01, 1, 100, 1.0e4, 1.0e6]:\n",
    "    print '\\nC = ' + str(c_candidate);\n",
    "    svm = SVC(kernel='rbf', C=c_candidate, random_state=0)\n",
    "    svm.fit(x_train, y_train)\n",
    "    Ein = countError(y_train, svm.predict(x_train));\n",
    "    Eout = countError(y_test, svm.predict(x_test));\n",
    "    NumSv = svm.n_support_[0]\n",
    "    print template.format(Ein, Eout, NumSv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lowest Ein comes from C = 10^6  \n",
    "Lowest Eout comes from C = 100  \n",
    "The larger C is, the more accurate and more likely to overfit is the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
